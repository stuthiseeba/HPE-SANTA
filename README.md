# HPE SANTA
LLM-as-a-Judge: RAG-Powered Document Query System
Overview
This project implements Retrieval-Augmented Generation (RAG) to enhance document querying with local LLMs, built using:
- LangGraph → Manages LLM workflows and decision-making.
- Qdrant → A high-performance vector database for fast retrieval.
- Ollama → Runs LLMs locally for inference.
- Streamlit → Provides an interactive UI for querying and visualization.

